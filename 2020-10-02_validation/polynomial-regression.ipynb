{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the convenience function *polynomial_features* that implements the polynomial feature mapping/transformation $\\boldsymbol{\\phi}(\\cdot)$ for a $D$-degree polynomial\n",
    "\n",
    "\\begin{align}\n",
    "    \\boldsymbol{\\phi}(x) & \\triangleq\n",
    "    \\begin{bmatrix}\n",
    "        x^D     \\\\\n",
    "        x^{D-1} \\\\\n",
    "        \\vdots  \\\\\n",
    "        x^2     \\\\\n",
    "        x       \\\\\n",
    "        1\n",
    "    \\end{bmatrix}    \n",
    "\\end{align}\n",
    "\n",
    "For a vector $\\mathbf{x} \\in \\mathbb{R}^N$ as input argument, it constructs and returns the matrix\n",
    "\n",
    "\\begin{align}\n",
    "    \\boldsymbol{\\Phi} & \\triangleq\n",
    "    \\begin{bmatrix}\n",
    "        \\boldsymbol{\\phi}^T(x_1) \\\\\n",
    "        \\boldsymbol{\\phi}^T(x_2) \\\\\n",
    "        \\vdots  \\\\\n",
    "        \\boldsymbol{\\phi}^T(x_N) \\\\ \n",
    "    \\end{bmatrix} =    \n",
    "    \\begin{bmatrix}\n",
    "        x_1^D & x_1^{D-1} & \\cdots & x_1^2 & x_1 & 1 \\\\\n",
    "        x_2^D & x_2^{D-1} & \\cdots & x_2^2 & x_2 & 1 \\\\\n",
    "        \\vdots  \\\\\n",
    "        x_N^D & x_N^{D-1} & \\cdots & x_N^2 & x_N & 1 \n",
    "    \\end{bmatrix} \\in \\mathbb{R}^{N \\times (D+1)}   \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, degree):\n",
    "    \"\"\"\n",
    "    Construct design matrix for polynomial regression of degree `polydegree`\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    x: numpy.ndarray, ndims=1\n",
    "        A 1-dimensional numpy array of length `N` contianing `x` values\n",
    "    \n",
    "    degree: int\n",
    "        An integer specifying the maximum degree of polynomial to be constructed\n",
    "        in the design matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Phi: np.ndarray, ndims=2\n",
    "        A 2-dimensional numpy array of shape `(N, polydegree+1)` representing the \n",
    "        design matrix. Columns are: \n",
    "        `[x**polydegree, x**(polydegree-1) ... x**2, x**1, 1]`\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    Phi = np.ones((N, degree + 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        Phi[:, -deg-1] = Phi[:, -deg] * x\n",
    "    \n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of true model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the true model to be the 3rd degree polynomial in $x$\n",
    "\n",
    "\\begin{align}\n",
    "    & f(x|\\mathbf{w}_{\\mathrm{true}}) = \\frac{A}{3} x^3 - \\frac{A(a+b)}{2} x^2 + A a b x = \\boldsymbol{\\phi}^T(x)  \\mathbf{w}_{\\mathrm{true}}\n",
    "\\end{align}\n",
    "\n",
    "where the polynomial coefficients $\\mathbf{w}_{\\mathrm{true}}$ are given as\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{w}_{\\mathrm{true}} & \\triangleq\n",
    "    \\begin{bmatrix}\n",
    "        \\frac{A}{3} \\\\\n",
    "        -\\frac{A (a+b)}{2} \\\\\n",
    "        a b A \\\\\n",
    "        0\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "and the polynomial feature map $\\boldsymbol{\\phi}(\\cdot)$ is given as\n",
    "\n",
    "\\begin{align}\n",
    "    \\boldsymbol{\\phi}(x) & \\triangleq\n",
    "    \\begin{bmatrix}\n",
    "        x^3 \\\\\n",
    "        x^2 \\\\\n",
    "        x   \\\\\n",
    "        1\n",
    "    \\end{bmatrix}    \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## True model (polynomial) specification\n",
    "\n",
    "# The parameters that follow influence the shape of the polynomial (feel free to change)\n",
    "a = 1.0    # First stationary point of polynomial\n",
    "b = 1.5    # Second stationary point of polynomial\n",
    "A = 2.0    # Scaling factor\n",
    "\n",
    "# Polynomial coefficients (do not change)\n",
    "w_true = np.zeros(4)\n",
    "w_true[0] = A / 3.0;\n",
    "w_true[1] = - A * (a + b) / 2.0\n",
    "w_true[2] = A * a * b\n",
    "w_true[3] = 0.2\n",
    "\n",
    "w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Data Generation Process\n",
    "\n",
    "We will feed the polynomial regression models noisy data obtained from the model above\n",
    "\n",
    "For any $x$ value $x_i$ we will compute the corresponding target $y_i$ as:\n",
    "\n",
    "\\begin{align*}\n",
    "y_i &= \\phi(x_i) \\cdot w_i  + \\epsilon_i, \\quad \\epsilon_i \\stackrel{\\text{iid}}{\\sim} N(0, \\sigma)\n",
    "\\end{align*}\n",
    "\n",
    "The parameter $\\sigma$ controls the strength of the noise\n",
    "\n",
    "Below we sample oa grid of x on $[0, 3]$, compute $\\phi(x)$, then add random noise to compute $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # for reproducible resultss\n",
    "\n",
    "N = 100\n",
    "xmin, xmax = 0, 3\n",
    "x = xmin + (xmax - xmin) * np.random.rand(N)\n",
    "\n",
    "\n",
    "sigma = 0.05\n",
    "epsilon = np.random.randn(100) * np.sqrt(sigma)\n",
    "\n",
    "## Construct the target values\n",
    "Phi = polynomial_features(x, 3)\n",
    "y = Phi @ w_true + epsilon\n",
    "\n",
    "# also construct data for plotting later\n",
    "x_plot = np.linspace(xmin, xmax, 40)\n",
    "y_true_plot = polynomial_features(x_plot, 3) @ w_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "\n",
    "The available data will be partitioned into a *training* set and a *hold-out* set (i.e. samples not used for training/fitting). In particular, the hold-out set will be used to obtain an honest estimate of how good the model performs on previously unseen samples (i.e. samples other than ones used for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partition data into training and holdout set\n",
    "\n",
    "# Number of training samples (feel free to change)\n",
    "N_train = 10\n",
    "\n",
    "N_holdout = N - N_train\n",
    "\n",
    "x_train = x[:N_train]\n",
    "y_train = y[:N_train]\n",
    "\n",
    "x_holdout = x[N_train:]\n",
    "y_holdout = y[N_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training/Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the degree of the polynomial that we plan to fit (feel free to change)\n",
    "polydegree = 3   # must be integer >=0 \n",
    "\n",
    "# Construct design matrix for polynomial regression (with intercept term)\n",
    "# for the training data\n",
    "Phi_train = polynomial_features(x_train, polydegree)\n",
    "\n",
    "# Fit/train the model by computing the optimal coefficient (weight) vector\n",
    "Phi_train_dagger = np.linalg.pinv(Phi_train)\n",
    "w_star = np.dot(Phi_train_dagger, y_train)\n",
    "\n",
    "# Compute the (optimal) predicted outputs for the training inputs\n",
    "y_hat_train = np.dot(Phi_train, w_star)\n",
    "\n",
    "# Compute the (minimum) MSE on the training set\n",
    "res_train = y_train - y_hat_train   # these are the optimal regression residuals\n",
    "MSE_train = np.linalg.norm(res_train,2)**2 / N_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-Out Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct design matrix for polynomial regression (with intercept term)\n",
    "# for the hold-out data\n",
    "Phi_holdout = polynomial_features(x_holdout, polydegree)\n",
    "\n",
    "# Compute the (optimal) predicted outputs for the hould-out inputs\n",
    "# using the optimal coefficients obtained by fitting the model\n",
    "y_hat_holdout = np.dot(Phi_holdout, w_star)\n",
    "\n",
    "# Compute the (minimum) MSE on the hold-out set\n",
    "res_holdout = y_holdout - y_hat_holdout  \n",
    "MSE_holdout = np.linalg.norm(res_holdout,2)**2 / N_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The optimal coefficients are:\\n')\n",
    "print(w_star)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('The (minimum) MSE of the fitted model as computed on the training set is\\nMSE_train={:.3f}\\n'.format(MSE_train))\n",
    "print('The MSE of the fitted model as computed on the hold-out set is\\nMSE_holdout={:.3f}\\n'.format(MSE_holdout))\n",
    "print('The noise variance of the true model is\\nsigmaSquared_e={:.3f}\\n'.format(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create data for plotting the fitted polynomial curve\n",
    "Phi_plot = polynomial_features(x_plot, polydegree)\n",
    "y_hat_plot_fitted = np.dot(Phi_plot, w_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize figure to make it bigger\n",
    "fig, ax = plt.subplots(figsize=(13, 8))\n",
    "\n",
    "ax.plot(x_plot, y_hat_plot_fitted, 'r', lw=3, label=\"Fitted model\")\n",
    "ax.plot(x_plot, y_true_plot, 'g', lw=3, label=\"True model\")\n",
    "ax.scatter(x_holdout, y_holdout, s=100, facecolors='none', edgecolors='b', label=\"Hold-Out data\")\n",
    "ax.scatter(x_train, y_train, s=100, facecolors='r', edgecolors='k', label=\"train data\")\n",
    "\n",
    "# Annotate graph\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title((\n",
    "    \"Fitted vs. True Model Responses: \"\n",
    "    f\"Ntrain={N_train:d} deg={polydegree:d} MSEtrain={MSE_train:.3f}, \"\n",
    "    f\"MSEholdout={MSE_holdout:.3f}, NoiseVar={sigma:.3f}\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn\n",
    "\n",
    "Below we will show an example of how to use scikit-learn to:\n",
    "\n",
    "1. construct the polynomial feature matrix\n",
    "2. Fit the polynomial regression\n",
    "3. Compute the MSE on training and holdout sets\n",
    "\n",
    "We will wrap this process up in a function that takes as an input the degree of the polynomial as well as the number of points to use for training\n",
    "\n",
    "This function will then be used to construct an interactive GUI where we will get sliders for the degree and number of training points, and as we adjust the sliders a plot of the model fit updates automatically\n",
    "\n",
    "Feel free to experiment with the two sliders and try to gain some intutions for when overfitting will and won't occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, linear_model, pipeline, metrics, model_selection\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyreg_demo(degree=8, n_train=10):\n",
    "    # define model\n",
    "    model = pipeline.make_pipeline(\n",
    "        preprocessing.PolynomialFeatures(degree=degree),\n",
    "        linear_model.LinearRegression()\n",
    "    )\n",
    "    \n",
    "    X = x[:, None]  # convert to 2d\n",
    "    test_size = X.shape[0] - n_train\n",
    "    split = model_selection.train_test_split(X, y, train_size=n_train, test_size=test_size, random_state=12)\n",
    "    X_train, X_test, y_train, y_test = split\n",
    "   \n",
    "    # fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(x_plot[:, None])\n",
    "    \n",
    "    # compute metrics\n",
    "    MSE_train = metrics.mean_squared_error(y_train, model.predict(X_train))\n",
    "    MSE_holdout = metrics.mean_squared_error(y_test, model.predict(X_test))\n",
    "\n",
    "    # make the plot\n",
    "    fig, ax = plt.subplots(figsize=(11, 8))\n",
    "    ax.plot(x_plot, yhat, \"k-\", lw=3, label=\"Fitted Model\")\n",
    "    ax.scatter(x_holdout, y_holdout, color=\"b\", s=60, alpha=0.5, label=\"Hold-Out Data\")\n",
    "    ax.scatter(X_train.flatten(), y_train, color=\"r\", s=80, alpha=0.7, label=\"Training Data\") \n",
    "    ax.set_ylim((-20, 20))\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_title((\n",
    "        \"Fitted vs. True Model Responses: \"\n",
    "        f\"Ntrain={N_train:d} deg={degree:d} MSEtrain={MSE_train:.3f}, \"\n",
    "        f\"MSEholdout={MSE_holdout:.3f}, NoiseVar={sigma:.3f}\"\n",
    "    ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "widgets.interactive(polyreg_demo, degree=(1, 10, 1), n_train=(5, 95, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = polyreg_demo(0, 10)\n",
    "fig.set_size_inches((11, 5))\n",
    "ax = fig.axes[0]\n",
    "ax.set_ylim((-2, 5))\n",
    "fig.savefig(\"underfit.pdf\")\n",
    "fig.savefig(\"../overfitting_model_selection/underfitting.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras (Optional)\n",
    "\n",
    "We have had a few questions about how Linear Regression relates to a neural network in keras\n",
    "\n",
    "In this section we will show how you can do Linear Regression using keras\n",
    "\n",
    "Note that this is like using a blowtorch when a match would do, but it will illustrate the following:\n",
    "\n",
    "- How you can view a LinearRegression as a `Dense` neural network layer\n",
    "\n",
    "- This will be the first example of an _iterative_ approach to finding coefficients for a linear regression. As mentioned in the lectures this the most numerically stable approach and often the only computationaly reasonable approach as either the number of samples or the number of features gets very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "Recall that a multi-layer-perceptron (MLP) is specific kind of neural network that is constructed by stacking one or more `Dense` layers, separated by non-linear activation functions\n",
    "\n",
    "An example of a 2 hidden layer MLP in keras is:\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(10), activation=\"relu\")  # hidden layer 1\n",
    "model.add(keras.layers.Dense(5), activation=\"relu\")   # hidden layer 2\n",
    "model.add(keras.layers.Dense(1), activation=None)     # output layer -- assuming we have 1 target to predict\n",
    "```\n",
    "\n",
    "Each dense layer in the network has the following structure: \n",
    "\n",
    "$$\\text{output} = \\mathbf{input} \\mathbf{W} + \\mathbf{b}$$ \n",
    "\n",
    "where $\\mathbf{input}$ is the input to the layer, $\\mathbf{W}$ is a matrix of weights or coefficients and $\\mathbf{b}$ is a vector of intercepts\n",
    "\n",
    "The number of columns in the matrix $\\mathbf{W}$ and the number of elements in the vector $\\mathbf{b}$ are the number of neurons in the hidden layer\n",
    "\n",
    "In our example, the $\\mathbf{W}$ for the first hidden layer would have shape $n_{\\text{features}} \\times 10$\n",
    "\n",
    "The output of this first hidden layer would he $n_{\\text{samples}} \\times 10$\n",
    "\n",
    "The $\\mathbf{W}$ for the second hidden layer would have shape $10 \\times 5$ -- there are $5$ rows because that is how many neurons there are in the hidden layer\n",
    "\n",
    "The $\\mathbf{b}$ fot the second layer would have $5$ elements and the output would be of shape $n_{\\text{samples}} \\times 5$, and so on until the last layer...\n",
    "\n",
    "Given an input sample $\\mathbf{x}$, we can compute the prediction of the network as follows:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output}_1 &= \\text{relu}(\\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1) \\\\\n",
    "\\text{output}_2 &= \\text{relu}(\\text{output}_1 \\mathbf{W}_2 + \\mathbf{b}_2) \\\\\n",
    "\\hat{y} &= \\text{output}_2 \\mathbf{W}_{\\text{final}} + \\mathbf{b}_{\\text{final}} \\\\\n",
    "        &= (\\text{relu}(\\text{relu}(\\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2) \\mathbf{W}_{\\text{out}} + \\mathbf{b}_{\\text{out}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case: Linear Regression\n",
    "\n",
    "We can consider a special case where there are no hidden layers and no activation function\n",
    "\n",
    "The keras model will be written as:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1), activation=None)   # output layer -- assuming we have 1 target to predict\n",
    "```\n",
    "\n",
    "and the prediction for an input sample $\\mathbf{x}$ is\n",
    "\n",
    "$$\\hat{y} = \\mathbf{x}\\mathbf{W} + \\mathbf{b},$$\n",
    "\n",
    "which is exactly our linear regression equation!\n",
    "\n",
    "Let's test this out with our polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1, use_bias=False, activation=None))   # output layer -- assuming we have 1 target to predict\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\")  # note we use mse as loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a degree 3 polynomial regression for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = polynomial_features(x_train, 3)\n",
    "X3_holdout = polynomial_features(x_holdout, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X3, y_train, epochs=10, validation_data=(X3_holdout, y_holdout));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that keras is reporting the `loss` after each pass through the training data\n",
    "\n",
    "This is the MSE with the current parameters\n",
    "\n",
    "It is changing because the parameters are being updated (stay tuned for later in the course and you'll learn _how_ theose parameters are being updated)\n",
    "\n",
    "Also, notice that the MSE after 10 epocs is much larger than the optimal MSE of 0.015 we found using the closed form optimal solution from linear regression \n",
    "\n",
    "Let's train the model for many more epochs and see if we can close that gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X3, y_train, epochs=4000, verbose=False, initial_epoch=11, validation_data=(X3_holdout, y_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x = np.arange(len(history.history[\"loss\"]))\n",
    "ax.plot(x, history.history[\"loss\"], label=\"train loss\")\n",
    "ax.plot(x, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax.hlines(MSE_train, xmin=x[0], xmax=x[-1], alpha=0.5, lw=4, color=\"red\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.legend()\n",
    "\n",
    "msg = \"The final MSE_train and MSE_validation are {} and {}\"\n",
    "print(msg.format(history.history[\"loss\"][-1], history.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training MSE is much closer to the optimum than before"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
